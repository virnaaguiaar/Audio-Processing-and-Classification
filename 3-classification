!nvidia-smi
!pip install --upgrade tensorflow keras

from google.colab import drive
drive.mount('/content/drive')

import tensorflow as tf
import os
import numpy as np
import cv2
import random
import pandas as pd
import seaborn as sns #heatmap
import matplotlib.pyplot as plt
from tensorflow_keras import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input
from tensorflow.keras.metrics import Precision, Recall
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from tqdm import tqdm


# Paths
train_dir = '/content/drive/MyDrive/audio2025/audios/spectrograms/train'
test_dir = '/content/drive/MyDrive/audio2025/audios/spectrograms/test'
validation_dir = '/content/drive/MyDrive/audio2025/audios/spectrograms/validation'

DATADIR = [train_dir, test_dir, validation_dir]
CATEGORIES = ["right", "left"]

# Create storage
train_data = []
test_data = []
validation_data = []

for dir in DATADIR:
  for category in CATEGORIES:
    # directory + class = complete path
    path = os.path.join(dir, category)
    # class index to divide/discriminate
    label = CATEGORIES.index(category)

    # Iterate through all files
    for img_file in tqdm(os.listdir(path), desc=f'Loading images from {category} - {DATADIR.index(dir)}', unit = 'image'):
      # Load image in grayscale
      img = cv2.imread(os.path.join(path, img_file), 1)
      # Resize image to 387x231 px
      img = cv2.resize(img, (387, 231))

      # Fill lists with files
      if(dir == train_dir):
        train_data.append([img, label])
      elif(dir == test_dir):
        test_data.append([img, label])
      elif(dir == validation_dir):
        validation_data.append([img, label])

# Data distribution overview
print(len(train_data))
print(len(test_data))
print(len(validation_data))

# Shuffle all files to avoid biased division (like input order in folder)
random.shuffle(train_data)
random.shuffle(test_data)
random.shuffle(validation_data)

# /255: normalize values and improve performance (speed up training and have more numerical stability)
# Images loaded in grayscale: ranges from [0,255]

# Extract only the image (spectrogram) using '_' to ignore 'label' (list = image + label)
X_train = np.array([spec for spec, _ in train_data]) / 255

# Extract only the label using '_' to ignore 'spectrogram' (list = image + label)
y_train = np.array([lab for _, lab in train_data])

X_test = np.array([spec for spec, _ in test_data]) / 255
y_test = np.array([lab for _, lab in test_data])
X_validation = np.array([spec for spec, _ in validation_data]) / 255
y_validation = np.array([lab for _, lab in validation_data])

# '.shape': output array dimensions
# Output: X(number of images, image dimensions, 3 = 3 color RGB)
#         y(number of images, )

print('Shapes:')
print('X_train:', X_train.shape)
print('y_train:', y_train.shape)
print('X_test:', X_test.shape)
print('y_test:', y_test.shape)
print('X_validation:', X_validation.shape)
print('y_validation:', y_validation.shape)


# Save multiple arrays in compressed .npz file
np.savez('/content/drive/MyDrive/audio2025/audios/test_validation_data.npz', X_train=X_train, y_test=y_test, X_validation=X_validation, y_validation=y_validation)

#Keras

# Define shape: (height, width, 3 = 3 color RGB)
input_shape = (231, 387, 3)

model = Sequential()

# Add input layer (so model knows input data dimension)
model.add(Input(shape=input_shape))

# ADD CONVOLUTIONAL LAYERS
# ---First layer (convolutional + pooling)
# (32 filters/kernels, filter size, common activator: REtified Linear Unit)
model.add(Conv2D(32, (3,3), activation='relu'))
# Takes 2x2 region from each image part - selects max value from that region - forms new reduced image
model.add(MaxPooling2D((2,2)))

#---Second layer (convolutional + pooling)
model.add(Conv2D(64, (3, 3), activation='relu'))
model.add(MaxPooling2D((2, 2)))

#---Third layer (convolutional + pooling)
model.add(Conv2D(128, (3, 3), activation='relu'))
model.add(MaxPooling2D((2, 2)))

# 'FLATTEN' 2D -> 1D
model.add(Flatten())

# ADD DENSE LAYERS
#---First dense layer + Dropout
# Layer with 512 neurons
model.add(Dense(512, activation='relu'))
# Randomly turns off 50% of neurons to avoid overfitting
model.add(Dropout(0.5))

#---Final layer (dense + Softmax)
model.add(Dense(6, activation='softmax'))



model.compile(optimizar='adam',
              loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),
              metrics=['accuracy'])


##
# Visão geral de todas as camadas do seu modelo, seus parâmetros, formas de saída e o número total de parâmetros treináveis e não treináveis
model.summary()  aquiiiiiiiiiiiiiii


##
# Treinar o modelo em Keras
# (dados de entrada pra aprender, rótulos de cada classe, treinar 10 vezes, dar acurácia ou a perda à cada treino/época)
historico = model.fit(X_treino, y_treino, epochs=10, validacao_dado=(X_validacao, y_validacao))


##
# classification_report: resumo completo das principais métricas de desempenho de um modelo de classificação - precisão, recall, F1-score, acurácia, para cada classe
# confusion_matrix: cria uma matriz de confusão - comparação entre as previsões do modelo e os rótulos reais (cada célula da matriz: número de ocorrências de cada combinação de rótulos previstos e reais)
# accuracy_score: (previsões corretas)/(total)



##
# Usa os dados testes para prever a saída
predictions = model.predict(X_teste)


##
# Mostrar(relatório detalhado(rótulo, retorna cada rótulo da na classe com a maior probabilidade))
print(classification_report(y_teste, predictions.argmax(axis=1)))


##

# Calcular a matrix de convolução(rótulos, previsões)
cm = confusion_matrix(y_teste, predictions.argmax(axis=1))

# Tamanho da figura (8x6 polegadas)
plt.figure(figsize=(8, 6))
#mapa de calor(matriz, adicionar os valores direto no mapa, tons de azul, decimal, eixos com as classes)
sns.heatmap(cm, annot=True, cmap='Blues', fmt='d', xticklabels=CATEGORIES, yticklabels=CATEGORIES)
plt.xlabel('Previsão')
plt.ylabel('Real')
plt.title('Matriz de confusão')
plt.savefig('/content/drive/MyDrive/audio2025/audios/confusion_matrix.png')
plt.show()



##
# Histórico de treinamento: acompanhar como acurácia e perda evoluíram ao longo das épocas
print(historico.history.keys())
print(historico.history['acuracy'])
print(historico.history['loss'])


##
# Gráfico de acurácia
plt.plot(history.history['accuracy'])
plt.title('Modelo de acurácia')
plt.ylabel('Acurácia')
plt.xlabel('Época')
plt.savefig('/content/drive/MyDrive/audio2025/audios/accuracy.png')


##
# Gráfico de perda
plt.plot(history.history['loss'])
plt.title('Modelo de perda')
plt.ylabel('Perda')
plt.xlabel('Época')
plt.savefig('/content/drive/MyDrive/audio2025/audios/loss.png')
plt.show()



# Acessa as métricas
metricas = historico.history
# Cria DataFrame com as métricas: uma tabela - coluna: métrica / linha: época
metricas_df = pd.DataFrame(metricas)
# to_csv(): converte o DataFrame metricas_df p-> arquivo CSV      //index=False: sem índice
metricas_df.to_csv('/content/drive/MyDrive/audio2025/audios/metrics_manualv2.csv', index=False)



##
# Converte o modelo treinado de TensorFlow/Keras -> TensorFlow Lite (TFLite)
converter = tf.lite.TFLiteConverter.from_keras_model(modelo)
tflite_model = converter.convert()

# Salvar o modelo
with open('/content/drive/MyDrive/audio2025/audios/model.tflite', 'wb') as f:
  f.write(tflite_model)
